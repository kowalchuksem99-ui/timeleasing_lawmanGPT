import pathlib
import uuid
import re
import time
import tqdm
import os

import tiktoken
from openai import OpenAI, OpenAIError          # ‚Üê —Ç–∏–ø –æ—à–∏–±–∫–∏ –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è
from qdrant_client import QdrantClient, models
from typing import Optional, List

# ‚Äì‚Äì‚Äì Parameters ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì
OPENAI_KEY  = (
    "API KEY"
)
QDRANT_KEY  = OPENAI_KEY          # –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –¥—Ä—É–≥–æ–π –∫–ª—é—á
QDRANT_HOST = "IP"
QDRANT_PORT = "PORT"

# === –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä (STEP_THREE) =========================
INDEX_POLL_SEC   = 120   # –±–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –æ–ø—Ä–æ—Å–∞, —Å–µ–∫
INDEX_MAX_BACKOFF = 900  # –º–∞–∫—Å–∏–º—É–º –±—ç–∫–æ—Ñ—Ñ–∞, —Å–µ–∫ (15 –º–∏–Ω—É—Ç)
FILE_STABLE_SEC   = 2    # —Ñ–∞–π–ª —Å—á–∏—Ç–∞–µ–º ¬´–≥–æ—Ç–æ–≤—ã–º¬ª, –µ—Å–ª–∏ –Ω–µ –º–µ–Ω—è–ª—Å—è >= N —Å–µ–∫

SRC_DIR   = r"C:\Users\User\Desktop\text_txt"
COLL      = "kad_cases"
EMB_MODEL = "MODEL"
DIM       = 768

CHUNK     = 800      # —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
OVERLAP   = 160      # –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
BATCH     = 128      # —Å–∫–æ–ª—å–∫–æ —Ç–æ—á–µ–∫ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞ —Ä–∞–∑

# –°—É—Ñ—Ñ–∏–∫—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω –ü–ï–†–ï–î —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º, –Ω–∞–ø—Ä–∏–º–µ—Ä
#   ¬´decision.txt¬ª ‚Üí ¬´decision.indexed.txt¬ª
PROCESSED_TAG = ".indexed"

CASE_RE = re.compile(
    r"(?:[–êA]\d{1,3}-\d{1,7}/\d{4}|–°–ò–ü-\d{1,7}(?:[-/]\d{4})?)",
    re.IGNORECASE
)
NUM_MAP = {
    "0": "–ù–û–õ–¨", "1": "–û–î–ò–ù", "2": "–î–í–ê", "3": "–¢–†–ò", "4": "–ß–ï–¢–´–†–ï",
    "5": "–ü–Ø–¢–¨", "6": "–®–ï–°–¢–¨", "7": "–°–ï–ú–¨", "8": "–í–û–°–ï–ú–¨", "9": "–î–ï–í–Ø–¢–¨",
    "-": "–¢–ò–†–ï", "/": "–°–õ–ï–®", "A": "–ê", "B": "–ë",
}

to_words = lambda s: " ".join(NUM_MAP.get(ch.upper(), ch) for ch in s)

# ‚Äì‚Äì‚Äì –ù–æ–≤–æ–µ: –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä –æ—à–∏–±–æ–∫ –±—é–¥–∂–µ—Ç–∞ ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì
class InsufficientFundsError(RuntimeError):
    """–ü—Ä–æ–∫–∏–¥—ã–≤–∞–µ—Ç—Å—è –Ω–∞–≤–µ—Ä—Ö, –µ—Å–ª–∏ —É –∞–∫–∫–∞—É–Ω—Ç–∞ –∑–∞–∫–æ–Ω—á–∏–ª–∏—Å—å —Å—Ä–µ–¥—Å—Ç–≤–∞."""

def is_insufficient_funds(exc: Exception) -> bool:     # NEW
    """
    –û—á–µ–Ω—å —Ä–∞–∑–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ openai-python —Ñ–æ—Ä–º–∏—Ä—É—é—Ç —Ç–µ–∫—Å—Ç—ã/–∫–æ–¥—ã –æ—à–∏–±–æ–∫
    –ø–æ-—Ä–∞–∑–Ω–æ–º—É, –ø–æ—ç—Ç–æ–º—É —Å–º–æ—Ç—Ä–∏–º –∏ –Ω–∞ code, –∏ –Ω–∞ —Ç–µ–∫—Å—Ç.
    """
    text = str(exc).lower()
    return any(tok in text for tok in ("insufficient", "quota", "balance"))

# ‚Äì‚Äì‚Äì Clients ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì
enc     = tiktoken.encoding_for_model(EMB_MODEL)
openai  = OpenAI(api_key=OPENAI_KEY)
qdrant  = QdrantClient(
    host=QDRANT_HOST,
    port=QDRANT_PORT,
    api_key=QDRANT_KEY,
    https=False,
    timeout=30.0,
)

# --- –®–∞–ø–∫–∞ –¥–µ–ª–∞: –°—É–¥ / –ò—Å—Ç–µ—Ü / –û—Ç–≤–µ—Ç—á–∏–∫ / –ù–æ–º–µ—Ä –¥–µ–ª–∞ -------------------------

HEADER_SLICE = 6000  # –∫–∞–∫ –±—ã–ª–æ

def _grab(label: str, head: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ–¥–Ω–æ–≥–æ –∏–∑ —è—Ä–ª—ã–∫–æ–≤ (label)
    –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —è—Ä–ª—ã–∫–∞ –∏–ª–∏ =====. –£—Å—Ç–æ–π—á–∏–≤–æ –∫ CRLF.
    """
    if not head:
        return None
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫: CRLF/CR -> LF
    head = head.replace("\r\n", "\n").replace("\r", "\n")

    # –í–ê–ñ–ù–û: –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ü–∏—é —è—Ä–ª—ã–∫–∞ (?:{label})
    pat = rf"(?im)^\s*(?:{label})\s*:\s*(.+?)\s*(?=\n\s*(?:–ù–æ–º–µ—Ä\s*–¥–µ–ª–∞|–°—É–¥|–ò—Å—Ç–µ—Ü|–û—Ç–≤–µ—Ç—á–∏–∫)\s*:|\n\s*={3,}|$)"
    m = re.search(pat, head)
    if not m:
        return None
    val = m.group(1)
    return val.strip() if isinstance(val, str) else None

def _clean_name(s: str) -> str:
    s = s.replace("\u00A0", " ")            # –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r"[¬´¬ª\"'‚Äú‚Äù]", "", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _split_many(s: Optional[str]) -> List[str]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–æ—Ä–æ–Ω –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã."""
    if not s:
        return []
    parts = re.split(r"\s*(?:;|,|\s+\b–∏\b\s*)\s*", s, flags=re.IGNORECASE)
    return [_clean_name(p) for p in parts if p and p.strip()]

CASE_IN_TEXT_RE = re.compile(
    r"(?im)^\s*–ù–æ–º–µ—Ä\s*–¥–µ–ª–∞\s*:\s*([–êA]\d{1,3}-\d{1,7}/\d{4}|–°–ò–ü-\d{1,7}(?:[-/]\d{4})?)"
)

def parse_header_fields(text: str) -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {'case_id', 'court', 'plaintiffs', 'defendants'}
    """
    if not text:
        return {"case_id": None, "court": None, "plaintiffs": [], "defendants": []}

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã –æ–¥–∏–Ω —Ä–∞–∑
    head = text[:HEADER_SLICE].replace("\r\n", "\n").replace("\r", "\n")

    m_case = CASE_IN_TEXT_RE.search(head)
    case_in_text = m_case.group(1).upper() if m_case else None

    court_raw = _grab(r"–°—É–¥", head)
    istec_raw = _grab(r"–ò—Å—Ç–µ—Ü|–ó–∞—è–≤–∏—Ç–µ–ª—å|–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω\w*\s+–∏—Å—Ç–µ—Ü", head)
    otv_raw   = _grab(r"–û—Ç–≤–µ—Ç—á–∏–∫|–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω\w*\s+–æ—Ç–≤–µ—Ç—á–∏–∫|–ó–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω\w*", head)

    court = _clean_name(court_raw) if court_raw else None
    plaintiffs = _split_many(istec_raw)
    defendants = _split_many(otv_raw)

    return {
        "case_id": case_in_text,
        "court": court,
        "plaintiffs": plaintiffs,
        "defendants": defendants,
    }


# ‚Äì‚Äì‚Äì Helper functions ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì

def chunker(text: str):
    """Yield overlapping chunks of text, each ‚âàCHUNK tokens."""
    tokens = enc.encode(text)
    step   = CHUNK - OVERLAP
    for i in range(0, len(tokens), step):
        yield enc.decode(tokens[i : i + CHUNK])


def extract_case(filename: str) -> str:
    m = CASE_RE.search(filename)
    return m.group(0).upper() if m else "UNKNOWN"


def mark_processed(path: pathlib.Path) -> pathlib.Path:
    """Return new Path with PROCESSED_TAG inserted **before** extension."""
    if path.suffix:  # ¬´file.txt¬ª ‚Üí ¬´file.indexed.txt¬ª
        return path.with_name(f"{path.stem}{PROCESSED_TAG}{path.suffix}")
    # —É —Ñ–∞–π–ª–∞ –Ω–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    return path.with_name(path.name + PROCESSED_TAG)


def _file_is_stable(path: pathlib.Path, stable_sec: int = FILE_STABLE_SEC) -> bool:
    """–§–∞–π–ª —Å—á–∏—Ç–∞–µ–º –≥–æ—Ç–æ–≤—ã–º, –µ—Å–ª–∏ –µ–≥–æ mtime —Å—Ç–∞—Ä—à–µ stable_sec."""
    try:
        mtime = os.path.getmtime(str(path))
        return (time.time() - mtime) >= stable_sec
    except FileNotFoundError:
        return False


def flush_batches(buf, *, wait=False):
    if buf:
        try:
            qdrant.upsert(COLL, points=buf, wait=wait)
        except Exception as exc:
            print(f"‚ö† Qdrant upsert failed (batch size {len(buf)}): {exc}")
        finally:
            buf.clear()


def ensure_payload_indexes():
    for field in ("case_id", "court", "plaintiffs", "defendants"):
        try:
            qdrant.create_payload_index(
                collection_name=COLL,
                field_name=field,
                field_schema=models.PayloadSchemaType.KEYWORD,
            )
        except Exception:
            pass  # —É–∂–µ –µ—Å—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Å—Ç—Å—è –ø–æ–∑–∂–µ

def ensure_collection():
    try:
        qdrant.get_collection(COLL)
    except Exception:
        print("‚è≥ –°–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é‚Ä¶")
        qdrant.create_collection(
            collection_name=COLL,
            vectors_config=models.VectorParams(size=DIM, distance=models.Distance.COSINE),
        )
    # ‚Üê –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã
    ensure_payload_indexes()


# ‚Äì‚Äì‚Äì Main indexing routine ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì

def index_all() -> int:
    """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤—Å–µ –ù–ï –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ TXT –∏–∑ SRC_DIR. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª-–≤–æ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤."""
    ensure_collection()
    points_buf = []
    processed_files = 0

    for path in tqdm.tqdm(pathlib.Path(SRC_DIR).glob("*.txt"), desc="–§–∞–π–ª—ã"):
        # –£–∂–µ –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ .indexed ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if path.name.endswith(PROCESSED_TAG + path.suffix):
            continue
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º ¬´–Ω–µ–¥–æ–ø–∏—Å–∞–Ω–Ω—ã–µ¬ª —Ñ–∞–π–ª—ã
        if not _file_is_stable(path, FILE_STABLE_SEC):
            continue

        filename = path.name
        case_num = extract_case(filename)
        raw_text = path.read_text(encoding="utf-8")

        info = parse_header_fields(raw_text)
        if case_num == "UNKNOWN" and info.get("case_id"):
            case_num = info["case_id"]

        court = info["court"]
        plaintiffs = info["plaintiffs"]
        defendants = info["defendants"]

        index_tag = f"<CASE:{case_num}>"
        if court:
            index_tag += f" <COURT:{court}>"
        if plaintiffs:
            index_tag += f" <ISTEC:{';'.join(plaintiffs[:2])}>"
        if defendants:
            index_tag += f" <OTV:{';'.join(defendants[:2])}>"
        index_tag += "\n"

        for chunk in chunker(raw_text):
            text_block = index_tag + chunk
            try:
                vec = openai.embeddings.create(
                    model=EMB_MODEL, input=text_block, dimensions=DIM
                ).data[0].embedding
            except OpenAIError as exc:
                if is_insufficient_funds(exc):
                    print("üí∏ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ä–µ–¥—Å—Ç–≤/–∫–≤–æ—Ç—ã OpenAI ‚Äî –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
                    raise InsufficientFundsError from exc
                print(f"‚ö† Embedding failed ({filename}): {exc}. –ß–∞–Ω–∫ –ø—Ä–æ–ø—É—â–µ–Ω.")
                continue
            except Exception as exc:
                print(f"‚ö† Embedding failed ({filename}): {exc}. –ß–∞–Ω–∫ –ø—Ä–æ–ø—É—â–µ–Ω.")
                continue

            points_buf.append(
                models.PointStruct(
                    id=str(uuid.uuid4()),
                    vector=vec,
                    payload={
                        "file": filename,
                        "text": text_block,
                        "case_id": case_num,
                        "court": court,
                        "plaintiffs": plaintiffs,
                        "defendants": defendants,
                    },
                )
            )

            if len(points_buf) == BATCH:
                flush_batches(points_buf)

        # ¬´—Ö–≤–æ—Å—Ç¬ª –ø–æ —Ñ–∞–π–ª—É –∏ –ø–æ–º–µ—Ç–∫–∞ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
        flush_batches(points_buf, wait=True)
        points_buf.clear()

        new_path = mark_processed(path)
        try:
            path.rename(new_path)
            processed_files += 1
            print(f"‚úî –û–±—Ä–∞–±–æ—Ç–∞–Ω: {new_path.name}")
        except Exception as exc:
            print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å {filename}: {exc}")

    if processed_files:
        print(f"üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ ‚Äî {processed_files}")
    else:
        print("‚Ñπ –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    return processed_files


# ‚Äì‚Äì‚Äì Auto-restart wrapper ‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì

def STEP_THREE(poll_sec: int = INDEX_POLL_SEC, max_backoff: int = INDEX_MAX_BACKOFF):
    """
    –î–µ–º–æ–Ω: –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–º–æ—Ç—Ä–∏—Ç –≤ SRC_DIR, –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã.
    –ï—Å–ª–∏ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç ‚Äî —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ø–∞—É–∑—É (—ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –±—ç–∫–æ—Ñ—Ñ) –¥–æ max_backoff.
    –ü—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö ‚Äî –ø–∞—É–∑–∞ —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è –∫ poll_sec.
    """
    retries = 0
    backoff = poll_sec
    while True:
        try:
            n = index_all()
            retries = 0
            if n == 0:
                time.sleep(backoff)
                backoff = min(backoff * 2, max_backoff)
            else:
                backoff = poll_sec
                time.sleep(poll_sec)

        except InsufficientFundsError:
            print("‚èπ –ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: –Ω—É–ª–µ–≤–æ–π –±–∞–ª–∞–Ω—Å/–∫–≤–æ—Ç–∞ OpenAI.")
            break

        except KeyboardInterrupt:
            print("‚èπ –ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä: –æ—Å—Ç–∞–Ω–æ–≤ –ø–æ Ctrl+C.")
            break

        except Exception as exc:
            wait = min(60, 2 ** retries)
            print(f"üí• STEP_THREE fatal: {exc!r}. –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ {wait} —Å–µ–∫‚Ä¶")
            time.sleep(wait)
            retries += 1

STEP_THREE()