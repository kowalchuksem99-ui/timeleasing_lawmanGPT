#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–°–±–æ—Ä–∫–∞ –¥–µ–ª –∏–∑ PDF –±–µ–∑ OCR:
- —á–∏—Ç–∞–µ—Ç –≤—Å–µ .pdf –∏–∑ SRC_DIR,
- –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç (PyMuPDF, —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π),
- –ø–∞—Ä—Å–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞,
- —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —à–∞–ø–∫—É,
- –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –Ω–æ–º–µ—Ä–æ–º –¥–µ–ª–∞ –≤ –æ–¥–∏–Ω .txt –≤ OUT_DIR.

–¢—Ä–µ–±—É–µ—Ç—Å—è: pip install pymupdf
"""

from __future__ import annotations

import os
import re
import sys
import unicodedata
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Dict

import fitz  # PyMuPDF


# ==================== –ù–ê–°–¢–†–û–ô–ö–ò ====================

SRC_DIR = r"C:\Users\User\Desktop\sorted_pdf"         # <- –ü–∞–ø–∫–∞ —Å PDF
OUT_DIR = r"C:\Users\User\Desktop\text_txt"          # <- –ö—É–¥–∞ –∫–ª–∞—Å—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–µ TXT

# –ö–æ–¥–∏—Ä–æ–≤–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Ç–æ–≥–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
OUT_ENCODING = "utf-8"

# –†–µ–≥—É–ª—è—Ä–∫–∞ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞:
#   - –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫—É—é –ê –∏ –ª–∞—Ç–∏–Ω—Å–∫—É—é A
#   - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –ø–µ—Ä–µ–¥ –≥–æ–¥–æ–º: -, / –∏–ª–∏ _
#   - –°–ò–ü-... —Å –≥–æ–¥–æ–º –∏–ª–∏ –±–µ–∑
CASE_RE = re.compile(
    r"(?:(?:[–êA]\d{1,3}-\d{1,7}[-/_]\d{4})|(?:–°–ò–ü-\d{1,7}(?:[-/_]\d{4})?))",
    re.IGNORECASE,
)

# –°–ø–ª–∏—Ç–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–º–µ–Ω–∏: " ‚Äî " (em/en/ascii dash c –ø—Ä–æ–±–µ–ª–∞–º–∏)
SEGMENT_SPLIT_RE = re.compile(r"\s+[‚Äî‚Äì-]\s+")

# –ó–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —Å—á–∏—Ç–∞–µ–º ¬´–ø—É—Å—Ç—ã –º–∏¬ª
EMPTY_TOKENS = {
    "", "–Ω/–¥", "–Ω–µ —É–∫–∞–∑–∞–Ω–æ", "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö",
    "—Å—É–¥ –Ω–µ –∏–∑–≤–ª–µ—á—ë–Ω", "—Å—É–¥_–Ω–µ_–∏–∑–≤–ª–µ—á—ë–Ω",
}


# ==================== –ú–û–î–ï–õ–ò ====================

@dataclass
class FileMeta:
    case_id: str | None
    court: str | None
    plaintiff: str | None
    defendants: List[str]
    filename: str
    text: str


@dataclass
class CaseBucket:
    case_id: str
    files: List[FileMeta] = field(default_factory=list)

    def merge_court(self) -> str | None:
        # –ø–µ—Ä–≤—ã–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π
        for fm in self.files:
            if fm.court and fm.court.lower() not in EMPTY_TOKENS:
                return fm.court
        return None

    def merge_plaintiff(self) -> str | None:
        for fm in self.files:
            if fm.plaintiff and fm.plaintiff.lower() not in EMPTY_TOKENS:
                return fm.plaintiff
        return None

    def merge_defendants(self) -> List[str]:
        bag: List[str] = []
        seen = set()
        for fm in self.files:
            for d in fm.defendants:
                key = d.lower()
                if key and key not in seen and key not in EMPTY_TOKENS:
                    seen.add(key)
                    bag.append(d)
        return bag


# ==================== –£–¢–ò–õ–ò–¢–´ ====================

def _norm_spaces(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\xa0", " ")
    s = re.sub(r"\s+", " ", s).strip(" ;,")
    return s


def _cleanup_entity(s: str | None) -> str | None:
    if not s:
        return None
    # —É–±–∏—Ä–∞–µ–º –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏—è –∫–∞–∫ ¬´–ø—Å–µ–≤–¥–æ-–∫—É—Ä—Å–∏–≤¬ª –∏–∑ –∏–º—ë–Ω (–û–û–û _–ù–¢–°-–†–ï–°–£–†–°_)
    s = s.replace("_", " ")
    s = _norm_spaces(s)
    return s or None


def _normalize_case(case_raw: str) -> str:
    """
    –ü—Ä–∏–≤–æ–¥–∏–º –Ω–æ–º–µ—Ä –¥–µ–ª–∞ –∫ –≤–∏–¥—É:
      –ê07-243/2020  (–ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ => /)
    –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–≥–∏—Å—Ç—Ä –±—É–∫–≤—ã '–ê' –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ (–∫–∏—Ä–∏–ª–ª/–ª–∞—Ç–∏–Ω –Ω–µ –º–µ–Ω—è–µ–º).
    """
    s = case_raw.strip()
    # —Ç–æ–ª—å–∫–æ –≤ –Ω–æ–º–µ—Ä–µ –¥–µ–ª–∞ '_' —Ç—Ä–∞–∫—Ç—É–µ–º –∫–∞–∫ '/'
    s = s.replace("_", "/")
    # –¥–≤–æ–π–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤ –µ–¥–∏–Ω–∏—á–Ω—ã–µ
    s = re.sub(r"[-/_]+", lambda m: m.group(0)[0], s)
    return s


def parse_filename(stem: str) -> tuple[str | None, str | None, str | None, List[str]]:
    """
    –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç:
        <case> ‚Äî <court> ‚Äî <plaintiff> ‚Äî <defendant(s)> ‚Äî ...
    –≥–¥–µ –æ—Ç–≤–µ—Ç—á–∏–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã —á–µ—Ä–µ–∑ ';'
    """
    parts = SEGMENT_SPLIT_RE.split(stem)
    parts = [p.strip() for p in parts if p is not None]

    case_id = None
    court = plaintiff = None
    defendants: List[str] = []

    # 1) –ù–û–ú–ï–† –î–ï–õ–ê ‚Äî –ø—ã—Ç–∞–µ–º—Å—è –≤—ã–Ω—É—Ç—å –∏–∑ 1-–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
    if parts:
        m = CASE_RE.search(parts[0])
        if m:
            case_id = _normalize_case(m.group(0))

    # 2) –°–£–î
    if len(parts) >= 2:
        court = _cleanup_entity(parts[1])

    # 3) –ò–°–¢–ï–¶
    if len(parts) >= 3:
        plaintiff = _cleanup_entity(parts[2])

    # 4) –û–¢–í–ï–¢–ß–ò–ö(–ò)
    if len(parts) >= 4:
        # –¥–µ–ª–∏–º –ø–æ ';'
        raw = parts[3]
        defs = [ _cleanup_entity(x) for x in raw.split(";") ]
        defendants = [d for d in defs if d]

    return case_id, court, plaintiff, defendants


def extract_pdf_text(pdf_path: Path) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –ë–ï–ó OCR (—Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π).
    –ï—Å–ª–∏ —É PDF –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞ (—Å–∫–∞–Ω), –≤–µ—Ä–Ω—ë—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É.
    """
    out_chunks: List[str] = []
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ç–æ–∫–æ–≤—ã–π ¬´–ø—Ä–æ—Å—Ç–æ–π¬ª —Ç–µ–∫—Å—Ç
                out_chunks.append(page.get_text("text"))
    except Exception as exc:
        print(f"‚ö† –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {pdf_path.name}: {exc}")
        return ""
    text = "\n".join(out_chunks)
    # –ü—Ä–∏–≤–æ–¥–∏–º –ø—Ä–æ–±–µ–ª—ã, —É–±–∏—Ä–∞–µ–º —Ö–≤–æ—Å—Ç–æ–≤—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    text = text.replace("\r", "")
    text = re.sub(r"[ \t]+\n", "\n", text)
    return text.strip()


def build_header(case_id: str | None, court: str | None,
                 plaintiff: str | None, defendants: List[str]) -> str:
    dlist = "; ".join(defendants) if defendants else "N/A"
    lines = [
        f"–ù–æ–º–µ—Ä –¥–µ–ª–∞: {case_id or 'N/A'}",
        f"–°—É–¥: {court or 'N/A'}",
        f"–ò—Å—Ç–µ—Ü: {plaintiff or 'N/A'}",
        f"–û—Ç–≤–µ—Ç—á–∏–∫: {dlist}",
    ]
    content = "\n".join(lines)
    border = "=" * 80
    title  = " –®–ê–ü–ö–ê –î–ï–õ–ê "
    # —Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —à–∏—Ä–∏–Ω—ã –≥—Ä–∞–Ω–∏—Ü—ã
    pad = max(0, (len(border) - len(title)) // 2)
    title_line = f"{'=' * pad}{title}{'=' * (len(border) - len(title) - pad)}"
    return f"{title_line}\n{content}\n{border}\n\n"



def ensure_out_dir() -> Path:
    out = Path(OUT_DIR)
    out.mkdir(parents=True, exist_ok=True)
    (out / "unknown").mkdir(parents=True, exist_ok=True)
    return out


def natural_key(s: str):
    return [int(t) if t.isdigit() else t.lower() for t in re.split(r"(\d+)", s)]

def safe_stem(name: str) -> str:
    """–î–µ–ª–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è –¥–ª—è —Ñ–∞–π–ª–∞ –Ω–∞ Windows/macOS/Linux."""
    # –∑–∞–º–µ–Ω—è–µ–º –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –Ω–∞ –¥–µ—Ñ–∏—Å
    s = re.sub(r'[\\/:*?"<>|]+', '-', name)
    # —É–±–∏—Ä–∞–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r'\s+', ' ', s).strip()
    # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –æ–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª–∏–Ω—É (NTFS –ª–∏–º–∏—Ç ~255 –±–∞–π—Ç)
    return s[:200]


# ==================== –û–°–ù–û–í–ù–û–ô –ü–†–û–¶–ï–°–° ====================

def STEP_TWO():
    src = Path(SRC_DIR)
    out_dir = ensure_out_dir()

    if not src.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {src}")
        sys.exit(1)

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–µ–ª—É
    buckets: Dict[str, CaseBucket] = {}
    singles: List[FileMeta] = []   # —Ñ–∞–π–ª—ã –±–µ–∑ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞

    pdf_files = sorted(
        (p for p in src.glob("*.pdf")),
        key=lambda p: natural_key(p.name)
    )

    if not pdf_files:
        print(f"‚ÑπÔ∏è  –í {SRC_DIR} –Ω–µ—Ç .pdf")
        return

    for pdf in pdf_files:
        stem = pdf.stem  # –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ .pdf
        case_id, court, plaintiff, defendants = parse_filename(stem)

        text = extract_pdf_text(pdf)
        if not text:
            print(f"‚ö† –í {pdf.name} –Ω–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π (–≤–æ–∑–º–æ–∂–Ω–æ —Å–∫–∞–Ω). –ü—Ä–æ–ø—É—â–µ–Ω.")
            continue

        meta = FileMeta(
            case_id=case_id,
            court=court if (court and court.lower() not in EMPTY_TOKENS) else None,
            plaintiff=plaintiff if (plaintiff and plaintiff.lower() not in EMPTY_TOKENS) else None,
            defendants=[d for d in defendants if d and d.lower() not in EMPTY_TOKENS],
            filename=pdf.name,
            text=text
        )

        if meta.case_id:
            b = buckets.setdefault(meta.case_id, CaseBucket(case_id=meta.case_id))
            b.files.append(meta)
        else:
            singles.append(meta)

    # –ü–∏—à–µ–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –¥–µ–ª–∞–º
    for case_id, bucket in buckets.items():
        bucket.files.sort(key=lambda fm: natural_key(fm.filename))
        court = bucket.merge_court()
        plaintiff = bucket.merge_plaintiff()
        defendants = bucket.merge_defendants()

        pieces: List[str] = []
        # –û–±—â–∞—è —à–∞–ø–∫–∞ –ø–æ –¥–µ–ª—É (–æ–¥–∏–Ω —Ä–∞–∑)
        pieces.append(build_header(case_id, court, plaintiff, defendants))

        # –ü—Ä–æ—Å—Ç–æ —Å–∫–ª–µ–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –≤—Å–µ—Ö PDF –±–µ–∑ –ø–æ–¥–ø–∏—Å–∏ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        for fm in bucket.files:
            pieces.append(fm.text.strip())
            pieces.append("\n\n")  # —Ä–∞–∑–¥–µ–ª—è–µ–º –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π

        content = "\n".join(pieces).strip() + "\n"
        out_path = out_dir / f"{safe_stem(case_id)}.txt"
        out_path.write_text(content, encoding=OUT_ENCODING)
        print(f"‚úî –°–æ–±—Ä–∞–Ω–æ –¥–µ–ª–æ: {out_path.name}  ({len(bucket.files)} PDF)")

    # –§–∞–π–ª—ã –±–µ–∑ –Ω–æ–º–µ—Ä–∞ –¥–µ–ª–∞ ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ –æ–¥–Ω–æ–º—É –≤ unknown/
    for fm in singles:
        header = build_header(None, fm.court, fm.plaintiff, fm.defendants)
        content = header + fm.text + "\n"
        out_path = out_dir / "unknown" / (safe_stem(Path(fm.filename).stem) + ".txt")
        out_path.write_text(content, encoding=OUT_ENCODING)
        print(f"‚úî –°–æ—Ö—Ä–∞–Ω—ë–Ω –±–µ–∑ –Ω–æ–º–µ—Ä–∞: unknown/{out_path.name}")

    print("üéâ –ì–æ—Ç–æ–≤–æ.")



STEP_TWO()